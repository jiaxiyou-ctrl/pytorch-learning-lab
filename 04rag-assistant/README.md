# 04 â€” RAG Assistant: Zombie Survival Guide

A Retrieval-Augmented Generation system that answers questions about a zombie survival guide using local LLM inference. The pipeline loads a document, splits it into chunks, embeds them into a vector store, retrieves relevant context, and generates answers with TinyLlama.

---

## Architecture

```
Document (.txt)
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Load    â”‚â”€â”€â”€â–¶â”‚  Split   â”‚â”€â”€â”€â–¶â”‚  Embed   â”‚â”€â”€â”€â–¶â”‚ Retrieve â”‚â”€â”€â”€â–¶â”‚ Generate â”‚
â”‚ TextLoaderâ”‚   â”‚ Recursiveâ”‚   â”‚ MiniLM   â”‚   â”‚ Chroma   â”‚   â”‚ TinyLlamaâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Tech Stack

| Component | Library |
|---|---|
| **Orchestration** | LangChain |
| **Vector Store** | ChromaDB |
| **Embeddings** | sentence-transformers (`all-MiniLM-L6-v2`) |
| **LLM** | TinyLlama 1.1B Chat (local, CPU) |
| **Document Loading** | LangChain `TextLoader` |

---

## Project Structure

```
04rag-assistant/
â”œâ”€â”€ main.py              # Entry point â€” run full pipeline
â”œâ”€â”€ README.md
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample.txt       # Zombie survival guide document
â””â”€â”€ src/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ step1_load.py     # Document loading
    â”œâ”€â”€ step2_split.py    # Text splitting
    â”œâ”€â”€ step3_embed.py    # Embedding & vector storage
    â”œâ”€â”€ step4_retrieve.py # Similarity search retrieval
    â””â”€â”€ step5_generate.py # LLM answer generation
```

---

## How to Run

```bash
# Create and activate environment
conda create -n rag_env python=3.11 -y
conda activate rag_env

# Install dependencies
pip install -r requirements.txt

# Run the full pipeline
python 04rag-assistant/main.py
```

> **Note:** The first run downloads the embedding model (~80 MB) and TinyLlama (~2 GB). Subsequent runs use cached models.

---

## Sample Output

```
============================================================
  ğŸ§Ÿ RAG Assistant: Zombie Survival Guide
============================================================

1. Loading documents...
  Document loaded successfully!
  Number of documents: 1

2.  Splitting into chunks...
  Documents split successfully!
  1 document(s) split into 6 chunks

3. Creating vector store...
  Vector database created successfully!
  Stored 6 vectors

4. Loading LLM...
  Loading model: TinyLlama/TinyLlama-1.1B-Chat-v1.0...
  Model loaded successfully!

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
--- Query 1/4 ---
â“ Q: What types of zombies are there?
ğŸ¤– A: Walkers are slow undead with strong bite force. Runners are
   recently turned with speed and aggression. Screamers attract
   nearby threats with piercing shrieks. Brutes can smash weak
   barricades in seconds.
ğŸ“š Sources: 3 chunks

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
--- Query 2/4 ---
â“ Q: What is the best weapon for close combat?
ğŸ¤– A: A sturdy crowbar is excellent â€” quiet, nearly unbreakable,
   good for prying and striking. Hatchets are compact and effective
   for controlled strikes.
ğŸ“š Sources: 3 chunks

============================================================
âœ… Done! Total execution time: 42.3s
============================================================
```

---

## Lessons Learned

- **RAG architecture** â€” Separating retrieval from generation lets a small LLM answer domain-specific questions it was never trained on
- **Chunking strategy** â€” Chunk size and overlap directly affect retrieval quality; too small loses context, too large dilutes relevance
- **Embeddings** â€” Sentence-transformers map semantically similar text to nearby vectors, enabling meaning-based search rather than keyword matching
- **Prompt engineering** â€” The chat template format (`<|system|>`, `<|user|>`, `<|assistant|>`) and explicit grounding instructions ("answer based ONLY on reference materials") significantly reduce hallucination
- **Local LLM inference** â€” TinyLlama runs on CPU without API keys, making the pipeline fully self-contained and reproducible

---

## References

- [LangChain Documentation](https://python.langchain.com/docs/get_started/introduction)
- [ChromaDB Documentation](https://docs.trychroma.com/)
- [Sentence-Transformers](https://www.sbert.net/)
- [TinyLlama](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)
